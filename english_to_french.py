# -*- coding: utf-8 -*-
"""English_to_french.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uN_JSZCTGudg37Q-zboY_kuSgdfFzC9m
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model

from google.colab import drive
drive.mount("/content/prj")

data1=pd.read_csv("prj/My Drive/small_vocab_en.csv",engine="python",header=None,sep="delimiter")
data2=pd.read_csv("prj/My Drive/small_vocab_fr.csv",engine="python",header=None,sep="delimiter",encoding="utf-8")
Data=pd.concat([data1,data2],axis=1)

def tokenize(x):
    token=Tokenizer(filters='')
    token.fit_on_texts(x)
    W=token.word_index
    seq=token.texts_to_sequences(x)
    seq=pad_sequences(seq,padding="post",truncating="post")
    return token,W,seq

T_e,W_e,seq_e=tokenize(list(Data.iloc[:,0]))
T_fr,W_fr,seq_fr=tokenize(list(Data.iloc[:,1]))

def a(x):
    return "<start> "+x+" <end>"
Data.iloc[:,0]=Data.iloc[:,0].apply(a)
Data.iloc[:,1]=Data.iloc[:,1].apply(a)

W_fr.get("<start>")

data=tf.data.Dataset.from_tensor_slices((seq_e,seq_fr))
data=data.shuffle(30000).batch(64,drop_remainder=True)

class Encoder(tf.keras.Model):
    def __init__(self,vocab):
        super().__init__()
        self.E=Embedding(vocab,100)
        self.L1=GRU(128,return_sequences=True,return_state=True)
    def __call__(self,I):
        x=self.E(I)
        x,h=self.L1(x)
        return x,h

W_fr.get("california")

class Attention(tf.keras.Model):
    def __init__(self,units):
        super().__init__()
        self.w1=Dense(units)
        self.w2=Dense(units)
        self.v=Dense(1)
    def __call__(self,EO,ES):
        ES=tf.expand_dims(ES,axis=1)
        score=self.v(tf.nn.tanh(self.w1(ES)+self.w2(EO)))
        attention=tf.nn.softmax(score,axis=1)
        context=attention*EO
        context=tf.reduce_sum(context,axis=1)
        return context

class Decoder(tf.keras.Model):
    def __init__(self,vocab):
        super().__init__()
        self.e=Embedding(vocab,100)
        self.L2=GRU(128,return_sequences=True,return_state=True)
        self.D1=Dense(vocab)
        self.a=Attention(128)
    def __call__(self,I,EO,ES):
        context=self.a(EO,ES)
        x=self.e(I)
        x=tf.concat([tf.expand_dims(context,1),x],axis=-1)
        x,h=self.L2(x)
        x=tf.reshape(x,(-1,x.shape[2]))
        x=self.D1(x)
        return x,h

opt=tf.keras.optimizers.Adam()
L=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
def loss_f(y,pred):
    mask=tf.math.logical_not(tf.math.equal(y,0))
    loss=L(y,pred)
    mask=tf.cast(mask,dtype=loss.dtype)
    loss*=mask
    return tf.reduce_mean(loss)





encoder=Encoder(len(W_e)+1)
decoder=Decoder(len(W_fr)+1)

@tf.function
def Prelims(I,T):
    loss=0
    with tf.GradientTape() as tape:
        EO,ES=encoder(I)
        D_H=ES
        l=[W_fr.get("<start>")]*64
        D_i=l
        D_i=tf.expand_dims(D_i,1)
        for i in range(1,T.shape[1]):
            DO,D_H=decoder(D_i,EO,D_H)
            loss+=loss_f(T[:,i],DO)
            D_i=tf.expand_dims(T[:,i],1)
        B=loss/int(T.shape[1])
        V=encoder.trainable_variables+decoder.trainable_variables
        grad=tape.gradient(loss,V)
        opt.apply_gradients(zip(grad,V))
        return B

##training
for i in range(20):
    total_loss=0
    for (b,(I,T)) in enumerate(data.take(400)):
        #print(I)
        L=Prelims(I,T)
        total_loss+=L
        if b%100==0:
            print(f'Epoch{i+1}---batch{b}and loss {L.numpy()}')

a1=seq_e.shape[1]
a2=seq_fr.shape[1]
def evaluate(sent):
    
    I=T_e.texts_to_sequences(sent)
    I=pad_sequences(I,maxlen=a1,padding="post",truncating="post")
    print(I)
    I=tf.convert_to_tensor(I)
    print(I)
    R=""
    EO,ES=encoder(I)
    DH=ES
    print(DH)
    DI=tf.expand_dims([W_fr.get('<start>')],0)
    print(DI)
    
    for i in range(a2):
        preds,DH=decoder(DI,EO,DH)
        id=tf.argmax(preds[0]).numpy()
        R+=T_fr.index_word.get(id)
        R+=" "
        print(R)
        if T_fr.index_word.get(id)=="<end>":
            return R
        DI=tf.expand_dims([id],0)

F=evaluate(["<start> this is life <end>"])

R=["the united states is it is cold in september ."]
I=T_e.texts_to_sequences(R)
I=pad_sequences(I,maxlen=a1,padding="post")
res=" "
EO,ES=encoder(I)
DH=ES
DI=tf.convert_to_tensor([W_fr.get('<start>')])
DI=tf.expand_dims(DI,0)
print(DI)
for i in range(a2):
    preds,DH=decoder(DI,EO,DH)
    id=tf.argmax(preds[0]).numpy()
    res+=T_fr.index_word.get(id)
    res+=" "
    if T_fr.index_word.get(id)=="<end>":
        break
    DI=tf.expand_dims([id],0)
print(res)



R=evaluate(["<start> you like pears , grapefruit , and limes .<end>"])

print(Data.iloc[150,0])
print(Data.iloc[150,1])

